{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "66032301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import pathlib as Path\n",
    "import random\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import pickle as pkl\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# from src_dataloadre import Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "55a1b960",
   "metadata": {},
   "outputs": [],
   "source": [
    "from legacy_dataloeders import TrainDataset, collate_fn, EvalDatasetGTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e826824a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# domains = ['mega', 'zvuk']\n",
    "# data_path = Path.Path('/home/jovyan/Samra/fl-recsys-adaptation/data/mega_zvuk-overlap50-minuser10-positives/')\n",
    "\n",
    "# domains = ['book', 'movie']\n",
    "# data_path = Path.Path('/home/jovyan/Samra/fl-recsys-adaptation/data/book_movie-overlap50-minuser10-positives')\n",
    "\n",
    "\n",
    "domains = ['sport', 'cloth']\n",
    "data_path = Path.Path('/home/jovyan/Samra/fl-recsys-adaptation/data/sport_cloth-overlap50-minuser10-all')\n",
    "\n",
    "# domains = ['phone', 'electronic']\n",
    "# data_path = Path.Path('/home/jovyan/Samra/fl-recsys-adaptation/data/phone_electronic-overlap50-minuser10-all')\n",
    "\n",
    "\n",
    "max_seq_len = 50\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "81841550",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_domain_data(data_path: Path):\n",
    "\n",
    "    train_df = (\n",
    "        pl.scan_parquet(data_path / 'train.parquet')\n",
    "        .group_by('uid')\n",
    "        .agg(pl.col('item_id'), pl.col('timestamp'))\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "    val_df = (\n",
    "        pl.scan_parquet(data_path / 'val.parquet')\n",
    "        .group_by('uid')\n",
    "        .agg(pl.col('item_id'), pl.col('timestamp'))\n",
    "        .collect()\n",
    "    )\n",
    "    test_df = (\n",
    "        pl.scan_parquet(data_path / 'test.parquet')\n",
    "        .group_by('uid')\n",
    "        .agg(pl.col('item_id'), pl.col('timestamp'))\n",
    "        .collect()\n",
    "    ).with_columns(pl.col('uid').cast(pl.Int32))\n",
    "    train_val_df = (pl.concat([\n",
    "        pl.scan_parquet(data_path / 'train.parquet'),\n",
    "        pl.scan_parquet(data_path / 'val.parquet'),\n",
    "        ])\n",
    "        .group_by('uid')\n",
    "        .agg(pl.col('item_id'), pl.col('timestamp'))\n",
    "        .collect()\n",
    "    ).with_columns(pl.col('uid').cast(pl.Int32))\n",
    "\n",
    "    with open(data_path / 'item_id_to_idx.pkl', 'rb') as f:\n",
    "        item_id_to_idx = pkl.load(f)\n",
    "\n",
    "    num_items = len(item_id_to_idx)\n",
    "\n",
    "    return train_df, val_df, train_val_df, test_df, num_items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "00b4f200",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dfs, val_dfs, train_val_dfs, test_dfs, num_items = [], [], [], [], []\n",
    "for domain in domains: \n",
    "    train_df, val_df, train_val_df, test_df, num_item = read_domain_data(data_path / domain)\n",
    "    train_dfs.append(train_df)\n",
    "    test_dfs.append(test_df)\n",
    "    train_val_dfs.append(train_val_df)\n",
    "    val_dfs.append(val_df)\n",
    "    num_items.append(num_item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ad136c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7089\n"
     ]
    }
   ],
   "source": [
    "all_users = np.union1d(train_dfs[0]['uid'].unique().to_numpy(), train_dfs[1]['uid'].unique().to_numpy())\n",
    "print(len(all_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "87267d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "uid_2_index = dict(zip(all_users, np.arange(len(all_users))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "685439d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dfs = [df.with_columns(pl.col('uid').replace_strict(uid_2_index)) for df in train_dfs]\n",
    "test_dfs = [df.with_columns(pl.col('uid').replace_strict(uid_2_index)) for df in test_dfs]\n",
    "val_dfs = [df.with_columns(pl.col('uid').replace_strict(uid_2_index)) for df in val_dfs]\n",
    "train_val_dfs = [df.with_columns(pl.col('uid').replace_strict(uid_2_index)) for df in train_val_dfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "008e36b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4726/4726 [00:12<00:00, 386.19it/s]\n",
      "100%|██████████| 4726/4726 [00:12<00:00, 391.56it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, domain in enumerate(domains):\n",
    "    os.makedirs(f'data/{domain}', exist_ok=True)\n",
    "\n",
    "    train_dataset = TrainDataset(dataset=train_dfs[i], num_items=num_items[i],\n",
    "                                    max_seq_len=max_seq_len, num_neg_items=1)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=1,\n",
    "        collate_fn=collate_fn,\n",
    "        drop_last=True,\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        prefetch_factor=4,\n",
    "    )\n",
    "    with open(f\"data/{domain}/train_data.txt\", \"w\") as f:\n",
    "        for batch in tqdm(train_dataloader):\n",
    "            user_id = int(batch[\"user.ids\"][0])\n",
    "            items = batch[\"item.ids\"].tolist()\n",
    "\n",
    "            f.write(str(user_id))\n",
    "            for item in items:\n",
    "                f.write(f\"\\t{int(item)}\")\n",
    "            f.write(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3d01ff08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_dataloader(train_df, eval_df, max_seq_len, batch_size, seed=42, eval_mode='random'):\n",
    "    \n",
    "    eval_df = train_df.join(eval_df, on='uid', how='inner', suffix='_valid').select(\n",
    "        pl.col('uid'), pl.col('item_id').alias('item_id_train'), pl.col('item_id_valid')\n",
    "    ).sort('uid')\n",
    "    eval_dataset = EvalDatasetGTS(dataset=eval_df, max_seq_len=max_seq_len, seed=seed, mode=eval_mode)\n",
    "\n",
    "    eval_dataloader = DataLoader(\n",
    "        dataset=eval_dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        drop_last=False,\n",
    "        num_workers=8,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    return eval_df, eval_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7b01af25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/977 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 977/977 [00:02<00:00, 359.87it/s]\n",
      "100%|██████████| 831/831 [00:02<00:00, 370.44it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, domain in enumerate(domains):\n",
    "    os.makedirs(f'data/{domain}', exist_ok=True)\n",
    "\n",
    "    eval_df, eval_dataloader = get_eval_dataloader(train_dfs[i], val_dfs[i], max_seq_len - 1, 1, eval_mode='last')\n",
    "    with open(f\"data/{domain}/valid_data.txt\", \"w\") as f:\n",
    "        for batch in tqdm(eval_dataloader):\n",
    "            user_id = int(batch[\"user.ids\"][0])\n",
    "            items = batch[\"item.ids\"].tolist()\n",
    "\n",
    "            f.write(str(user_id))\n",
    "            for item in items:\n",
    "                f.write(f\"\\t{int(item)}\")\n",
    "\n",
    "            f.write(f\"\\t{batch['labels.ids'][0]}\")\n",
    "            f.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0ee499ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/748 [00:00<01:24,  8.86it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 748/748 [00:02<00:00, 349.49it/s]\n",
      "100%|██████████| 748/748 [00:02<00:00, 352.42it/s]\n",
      "100%|██████████| 748/748 [00:02<00:00, 356.90it/s]\n",
      "100%|██████████| 642/642 [00:01<00:00, 351.00it/s]\n",
      "100%|██████████| 642/642 [00:01<00:00, 364.11it/s]\n",
      "100%|██████████| 642/642 [00:01<00:00, 362.05it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, domain in enumerate(domains):\n",
    "    os.makedirs(f'data/{domain}', exist_ok=True)\n",
    "    with open(f\"data/{domain}/test_data.txt\", \"w\") as f:\n",
    "        for seed in [42, 41, 43]:\n",
    "            eval_df, eval_dataloader = get_eval_dataloader(train_val_dfs[i], test_dfs[i], max_seq_len - 1, batch_size=1, eval_mode='random', seed=seed)\n",
    "            for batch in tqdm(eval_dataloader):\n",
    "                user_id = int(batch[\"user.ids\"][0])\n",
    "                items = batch[\"item.ids\"].tolist()\n",
    "\n",
    "                f.write(str(user_id))\n",
    "                for item in items:\n",
    "                    f.write(f\"\\t{int(item)}\")\n",
    "                f.write(f\"\\t{batch['labels.ids'][0]}\")\n",
    "                f.write(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ef967b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, domain in enumerate(domains):\n",
    "    os.makedirs(f'data/{domain}', exist_ok=True)\n",
    "    with open(f\"data/{domain}/num_items.txt\", \"w\") as f:\n",
    "        f.write(f'{num_items[i]}')\n",
    "    with open(f\"data/{domain}/item_users.txt\", \"w\") as f:\n",
    "        f.write(f'{len(train_dfs[i])}')\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a6df1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FedCDR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
